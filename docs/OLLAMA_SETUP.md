# Настройка Ollama для генерации инструкций

## Вариант 1: Локальный Ollama (бесплатно)

1. **Установите Ollama:**
   - Скачайте с https://ollama.ai/
   - Установите и запустите

2. **Скачайте быструю легкую модель (рекомендуется):**
   ```bash
   ollama pull llama3.2:1b
   ```
   Эта модель очень быстрая (1 миллиард параметров) и хорошо работает для генерации текста.

   **Альтернативные модели:**
   ```bash
   ollama pull phi-3-mini      # Маленькая и быстрая
   ollama pull mistral:7b      # Средняя, хорошее качество
   ollama pull llama3.2:3b     # Немного больше, но все еще быстрая
   ```

3. **Настройте переменные окружения** (создайте файл `backend/.env`):
   ```
   OLLAMA_URL=http://localhost:11434
   OLLAMA_MODEL=llama3.2:1b
   USE_OLLAMA_CLOUD=false
   ```

4. **Перезапустите backend**

## Вариант 2: Ollama Cloud API (платно, но удобнее)

1. **Получите API ключ:**
   - Зарегистрируйтесь на https://ollama.ai/
   - Получите API ключ в личном кабинете

2. **Настройте переменные окружения** (создайте файл `backend/.env`):
   ```
   USE_OLLAMA_CLOUD=true
   OLLAMA_API_KEY=ваш_api_ключ_здесь
   OLLAMA_MODEL=llama3.2:1b
   ```

3. **Перезапустите backend**

## Рекомендуемые модели (по скорости и качеству):

- **llama3.2:1b** - очень быстрая легкая модель (1B параметров) - **РЕКОМЕНДУЕТСЯ**
- **phi-3-mini** - маленькая и быстрая модель от Microsoft
- **mistral:7b** - средняя модель, хорошее качество и скорость
- **llama3.2:3b** - немного больше, но все еще быстрая

**Примечание:** Модели с меньшим количеством параметров генерируют быстрее, но могут давать менее детальный текст. Для инструкций по охране труда llama3.2:1b обычно достаточно.

## Проверка работы:

После настройки при генерации инструкции вы увидите:
- ✅ "Текст успешно сгенерирован с помощью ИИ (Ollama)" - если все работает
- ⚠️ "Текст сгенерирован шаблоном (Ollama недоступен)" - если есть проблемы

